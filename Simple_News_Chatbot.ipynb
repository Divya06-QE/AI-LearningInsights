{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divya06-QE/AI-LearningInsights/blob/main/Simple_News_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a simple, all-in-one script for a news chatbot.\n",
        "# It handles everything from scraping news to answering your questions with AI.\n",
        "# The code is designed to be easy to follow for non-coders.\n",
        "\n",
        "# --- 1. Install necessary libraries if they are not already installed ---\n",
        "# The code below will check for and install the required libraries.\n",
        "# This prevents the \"ModuleNotFoundError\" you encountered.\n",
        "try:\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    import nltk\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from pymongo import MongoClient\n",
        "    from openai import OpenAI\n",
        "    import dotenv\n",
        "    import time\n",
        "    import json\n",
        "    import os\n",
        "except ImportError:\n",
        "    print(\"One or more required modules not found. Installing now...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    required_packages = [\n",
        "        \"requests\",\n",
        "        \"beautifulsoup4\",\n",
        "        \"nltk\",\n",
        "        \"scikit-learn\",\n",
        "        \"pymongo\",\n",
        "        \"openai\",\n",
        "        \"python-dotenv\"\n",
        "    ]\n",
        "\n",
        "    for package in required_packages:\n",
        "        try:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Failed to install {package}. Error: {e}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "    print(\"\\n--- Installation Complete ---\")\n",
        "    print(\"All required packages installed successfully.\")\n",
        "    print(\"Please run this script again to continue.\")\n",
        "    sys.exit(0)\n",
        "\n",
        "# --- 2. Import all the necessary tools (Dependencies) ---\n",
        "# These are like the building blocks we need for our project.\n",
        "# `requests` helps us download web pages.\n",
        "import requests\n",
        "# `BeautifulSoup` helps us read and find information in web pages.\n",
        "from bs4 import BeautifulSoup\n",
        "# `nltk` is a library for working with human language.\n",
        "import nltk\n",
        "# These specific parts of `nltk` help us find common words and break sentences into words.\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# `time` lets us pause the program for a moment.\n",
        "import time\n",
        "# `json` helps us work with data in a format a lot of programs understand.\n",
        "import json\n",
        "# `os` helps us get information from our computer, like API keys.\n",
        "import os\n",
        "# `dotenv` helps us read our secret keys from a special file named \".env\".\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# For data processing and finding similar articles\n",
        "# `TfidfVectorizer` is a tool that turns words into numbers.\n",
        "# It helps the computer understand how important a word is in a document.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# `cosine_similarity` is a math tool that measures how similar two sets of numbers are.\n",
        "# We use it to find out if two articles are about the same topic.\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# For connecting to our database\n",
        "# `MongoClient` is the main tool we use to talk to our MongoDB database.\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# For our AI chatbot\n",
        "# `OpenAI` is the library we use to talk to the powerful AI models from OpenAI.\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- 3. Load API Keys from a secure file ---\n",
        "# This loads your private keys from a file named \".env\". This is safer than putting them directly in the code.\n",
        "load_dotenv()\n",
        "# We get the keys from the environment variables.\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
        "\n",
        "# --- 4. Set up our AI and Database connections ---\n",
        "# We create a client to talk to the OpenAI service. If the key is missing, we'll see an error message.\n",
        "if not OPENAI_API_KEY:\n",
        "    print(\"Error: Missing OpenAI API key. Please add it to your .env file.\")\n",
        "else:\n",
        "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# We create a client to talk to our MongoDB database.\n",
        "if not MONGO_URI:\n",
        "    print(\"Error: Missing MongoDB URI. Please add it to your .env file.\")\n",
        "else:\n",
        "    try:\n",
        "        mongo_client = MongoClient(MONGO_URI)\n",
        "        db = mongo_client.news_db\n",
        "        highlights_collection = db.highlights\n",
        "        print(\"Connected to MongoDB successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error connecting to MongoDB: {e}\")\n",
        "        mongo_client = None\n",
        "        db = None\n",
        "        highlights_collection = None\n",
        "\n",
        "# --- 5. Prepare the AI and Text tools ---\n",
        "# We download the text processing models from NLTK.\n",
        "try:\n",
        "    # This checks if the 'punkt' and 'stopwords' models are already on your computer.\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    print(\"Required NLTK packages not found. Downloading now...\")\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    print(\"NLTK packages downloaded successfully.\")\n",
        "\n",
        "# --- 6. Define where we get our news from ---\n",
        "# This is a list of news categories and the websites we will scrape.\n",
        "NEWS_SOURCES = {\n",
        "    'sports': 'https://www.theguardian.com/au/sport',\n",
        "    'lifestyle': 'https://7news.com.au/lifestyle',\n",
        "    'music': 'https://7news.com.au/entertainment/music',\n",
        "    'finance': 'https://www.theguardian.com/au/business/finance'\n",
        "}\n",
        "\n",
        "# --- 7. Helper Functions (The magic happens here) ---\n",
        "\n",
        "def get_articles_from_source(category, url):\n",
        "    \"\"\"\n",
        "    This function visits a news website and scrapes the articles.\n",
        "    It looks for titles and summaries and puts them into a list.\n",
        "    \"\"\"\n",
        "    articles = []\n",
        "    try:\n",
        "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        article_elements = soup.find_all(['article', 'div'], class_=['media-body', 'c-card', 'c-card__content'])\n",
        "\n",
        "        for elem in article_elements[:10]:\n",
        "            title_tag = elem.find(['h2', 'h3', 'h4', 'a'], class_=['media-heading', 'c-card__headline', 'headline'])\n",
        "            summary_tag = elem.find(['p', 'div'], class_=['media-description', 'c-card__summary', 'description'])\n",
        "            if title_tag:\n",
        "                title = title_tag.get_text(strip=True)\n",
        "                summary = summary_tag.get_text(strip=True) if summary_tag else \"No summary available.\"\n",
        "                link = title_tag.find_parent('a')['href'] if title_tag.find_parent('a') else \"No link.\"\n",
        "                author_tag = elem.find(class_='byline')\n",
        "                author = author_tag.get_text(strip=True) if author_tag else \"Unknown Author\"\n",
        "                articles.append({\n",
        "                    'category': category,\n",
        "                    'source': url,\n",
        "                    'title': title,\n",
        "                    'summary': summary,\n",
        "                    'author': author,\n",
        "                    'link': link\n",
        "                })\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "    return articles\n",
        "\n",
        "def find_duplicates_and_cluster(articles):\n",
        "    \"\"\"\n",
        "    This function finds and groups articles that are about the same story.\n",
        "    It uses a technique called TF-IDF and a similarity score (cosine similarity).\n",
        "    Think of it as finding duplicates even if the words are slightly different.\n",
        "    \"\"\"\n",
        "    if not articles:\n",
        "        return []\n",
        "    corpus = [a['title'] + \" \" + a['summary'] for a in articles]\n",
        "    cleaned_corpus = [doc for doc in corpus if doc.strip()]\n",
        "    if not cleaned_corpus:\n",
        "        return articles\n",
        "\n",
        "    # Converts text into numbers the computer can understand\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(cleaned_corpus)\n",
        "    # Measures how similar the articles are (0 = not similar, 1 = identical)\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    clusters = []\n",
        "    processed_indices = set()\n",
        "    for i in range(len(cosine_sim)):\n",
        "        if i not in processed_indices:\n",
        "            cluster = [articles[i]]\n",
        "            processed_indices.add(i)\n",
        "            for j in range(i + 1, len(cosine_sim)):\n",
        "                if cosine_sim[i][j] > 0.5 and j not in processed_indices:\n",
        "                    cluster.append(articles[j])\n",
        "                    processed_indices.add(j)\n",
        "            clusters.append(cluster)\n",
        "    return clusters\n",
        "\n",
        "def get_highlights_from_clusters(clusters):\n",
        "    \"\"\"\n",
        "    This function turns the article groups into a final list of top highlights.\n",
        "    It prioritizes stories that were reported by the most sources.\n",
        "    \"\"\"\n",
        "    highlights = []\n",
        "    for cluster in clusters:\n",
        "        main_article = cluster[0]\n",
        "        summary = main_article['summary']\n",
        "        sources = list(set([a['source'] for a in cluster]))\n",
        "        authors = list(set([a['author'] for a in cluster]))\n",
        "\n",
        "        highlights.append({\n",
        "            'title': main_article['title'],\n",
        "            'summary': summary,\n",
        "            'sources': sources,\n",
        "            'authors': authors,\n",
        "            'frequency': len(cluster)\n",
        "        })\n",
        "    # Sorts the highlights so the most frequent stories are at the top\n",
        "    return sorted(highlights, key=lambda x: x['frequency'], reverse=True)\n",
        "\n",
        "def save_highlights(highlights_data):\n",
        "    \"\"\"\n",
        "    This function saves the processed news to our MongoDB database.\n",
        "    It first clears the old news and then saves the new highlights.\n",
        "    \"\"\"\n",
        "    if highlights_collection:\n",
        "        highlights_collection.delete_many({})\n",
        "        for category, highlights_list in highlights_data.items():\n",
        "            for highlight in highlights_list:\n",
        "                highlight['category'] = category\n",
        "                highlights_collection.insert_one(highlight)\n",
        "        print(\"Highlights saved to MongoDB!\")\n",
        "    else:\n",
        "        print(\"Could not save to MongoDB. Is your connection working?\")\n",
        "\n",
        "def find_relevant_highlights(query):\n",
        "    \"\"\"\n",
        "    This function looks for news in our database that matches your question.\n",
        "    It finds articles where the title or summary contains your keywords.\n",
        "    \"\"\"\n",
        "    if not highlights_collection:\n",
        "        return []\n",
        "\n",
        "    # We look for documents that have your query in the title or summary.\n",
        "    # The `\"$regex\"` part allows us to find partial matches.\n",
        "    relevant_docs = list(highlights_collection.find({\n",
        "        \"$or\": [\n",
        "            {\"title\": {\"$regex\": query, \"$options\": \"i\"}},\n",
        "            {\"summary\": {\"$regex\": query, \"$options\": \"i\"}}\n",
        "        ]\n",
        "    }))\n",
        "    return relevant_docs[:5]\n",
        "\n",
        "def answer_query(query):\n",
        "    \"\"\"\n",
        "    This is our chatbot. It gets your question and finds a smart answer.\n",
        "    1. It first retrieves relevant information from our database.\n",
        "    2. Then, it sends that information to the AI model along with your question.\n",
        "    3. The AI uses the provided information to generate a helpful response.\n",
        "    \"\"\"\n",
        "    if not openai_client:\n",
        "        return \"I can't answer your question because the OpenAI client is not set up.\"\n",
        "\n",
        "    # Step 1: Retrieval (getting the info)\n",
        "    relevant_highlights = find_relevant_highlights(query)\n",
        "\n",
        "    if not relevant_highlights:\n",
        "        return \"I'm sorry, I can't find any news about that topic.\"\n",
        "\n",
        "    # Step 2: Augmentation (preparing the info for the AI)\n",
        "    # We turn the database results into a clear message for the AI.\n",
        "    context = json.dumps(relevant_highlights, indent=2)\n",
        "    prompt = f\"\"\"\n",
        "    Answer the following question based *only* on the provided news highlights. If the information is not present, state that you don't know.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 3: Generation (getting the answer)\n",
        "    try:\n",
        "        # This is where we talk to the OpenAI model. We send our prompt as a 'user' message.\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",  # The specific AI model we are using\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant who answers questions based on a provided context.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error during AI model call: {e}\")\n",
        "        return \"I'm having trouble connecting to the AI model right now.\"\n",
        "\n",
        "# --- 8. The main part of the program that runs everything ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Welcome to the Simple News Chatbot!\")\n",
        "    print(\"\\n--- Let's get things ready ---\")\n",
        "    print(\"Checking for required libraries and NLTK data...\")\n",
        "\n",
        "    # We download the text processing models from NLTK.\n",
        "    try:\n",
        "        # This checks if the 'punkt' and 'stopwords' models are already on your computer.\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "        print(\"All NLTK packages are already installed!\")\n",
        "    except LookupError:\n",
        "        print(\"Required NLTK packages not found. Downloading now...\")\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "        print(\"NLTK packages downloaded successfully.\")\n",
        "\n",
        "    print(\"\\n--- Starting the news scraping process ---\")\n",
        "\n",
        "    # Get all the news articles from our sources\n",
        "    all_articles = []\n",
        "    for category, url in NEWS_SOURCES.items():\n",
        "        print(f\"Scraping {category} from {url}...\")\n",
        "        articles = get_articles_from_source(category, url)\n",
        "        all_articles.extend(articles)\n",
        "        time.sleep(1) # Wait a moment to be polite to the websites\n",
        "\n",
        "    if all_articles:\n",
        "        print(\"\\nFinding and grouping similar news stories...\")\n",
        "        categorized_articles = {cat: [] for cat in NEWS_SOURCES.keys()}\n",
        "        for article in all_articles:\n",
        "            categorized_articles[article['category']].append(article)\n",
        "\n",
        "        news_highlights = {}\n",
        "        for category, articles in categorized_articles.items():\n",
        "            clusters = find_duplicates_and_cluster(articles)\n",
        "            highlights = get_highlights_from_clusters(clusters)\n",
        "            news_highlights[category] = highlights\n",
        "\n",
        "        # Save the processed news to our database\n",
        "        save_highlights(news_highlights)\n",
        "\n",
        "        print(\"\\nNews highlights are ready!\")\n",
        "    else:\n",
        "        print(\"\\nNo articles were scraped. Please check your internet connection or news sources.\")\n",
        "\n",
        "    print(\"\\n--- Start the Chatbot ---\")\n",
        "    print(\"You can ask questions about the news highlights. Type 'exit' to quit.\")\n",
        "\n",
        "    # The chatbot loop\n",
        "    while True:\n",
        "        user_query = input(\"\\nAsk a question: \")\n",
        "        if user_query.lower() == 'exit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Get the AI to answer our question\n",
        "        response = answer_query(user_query)\n",
        "        print(f\"\\nAI: {response}\")"
      ],
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Missing OpenAI API key. Please add it to your .env file.\n",
            "Error: Missing MongoDB URI. Please add it to your .env file.\n",
            "Welcome to the Simple News Chatbot!\n",
            "\n",
            "--- Let's get things ready ---\n",
            "Checking for required libraries and NLTK data...\n",
            "All NLTK packages are already installed!\n",
            "\n",
            "--- Starting the news scraping process ---\n",
            "Scraping sports from https://www.theguardian.com/au/sport...\n",
            "Scraping lifestyle from https://7news.com.au/lifestyle...\n",
            "Scraping music from https://7news.com.au/entertainment/music...\n",
            "Scraping finance from https://www.theguardian.com/au/business/finance...\n",
            "Error scraping https://www.theguardian.com/au/business/finance: 404 Client Error: Not Found for url: https://www.theguardian.com/au/business/finance\n",
            "\n",
            "No articles were scraped. Please check your internet connection or news sources.\n",
            "\n",
            "--- Start the Chatbot ---\n",
            "You can ask questions about the news highlights. Type 'exit' to quit.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZLtZ111aknc",
        "outputId": "a0ab7e16-0bcd-4d40-b468-6fb34fdf51c3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}